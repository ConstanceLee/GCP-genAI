{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojoyvz6mH1Hv"
      },
      "source": [
        "# Supervised Fine Tuning with Gemini 1.5 Flash for Q&A\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fsupervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Gemini** is a family of generative AI models developed by Google DeepMind designed for multimodal use cases. The Gemini API gives you access to the various Gemini models, such as Gemini 1.5 Pro and Gemini 1.5 Flash.\n",
        "This notebook demonstrates fine-tuning the Gemini 1.5 Flahs using the Vertex AI Supervised Tuning feature. Supervised Tuning allows you to use your own labeled training data to further refine the base model's capabilities toward your specific tasks.\n",
        "Supervised Tuning uses labeled examples to tune a model. Each example demonstrates the output you want from your text model during inference.\n",
        "First, ensure your training data is of high quality, well-labeled, and directly relevant to the target task. This is crucial as low-quality data can adversely affect the performance and introduce bias in the fine-tuned model.\n",
        "Training: Experiment with different configurations to optimize the model's performance on the target task.\n",
        "Evaluation:\n",
        "Metric: Choose appropriate evaluation metrics that accurately reflect the success of the fine-tuned model for your specific task\n",
        "Evaluation Set: Use a separate set of data to evaluate the model's performance\n",
        "\n",
        "\n",
        "Refer to public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning) for more details.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "\n",
        "- A Google Cloud project: Provide your project ID in the `PROJECT_ID` variable.\n",
        "\n",
        "- Authenticated your Colab environment: Run the authentication code block at the beginning.\n",
        "\n",
        "- Prepared training data (Test with your own data or use the one in the notebook): Data should be formatted in JSONL with prompts and corresponding completions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7SS5pzuIA-1"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage.\n",
        "\n",
        "To get an estimate of the number of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRvKdaPDTznN",
        "outputId": "b9219138-e8f3-4cfd-e324-9d61ef383732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"<your_project_id>\"  # @param {type:\"string\", isTemplate: true}\n",
        "if PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# Vertex AI SDK\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.metadata import context\n",
        "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from vertexai.preview.tuning import sft\n",
        "\n",
        "# Vertex AI SDK\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import array\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bBZa2I-c-x8"
      },
      "source": [
        "### Data\n",
        "\n",
        "#### SQuAD dataset\n",
        "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "You can fine more information on the SQuAD [github page](https://rajpurkar.github.io/SQuAD-explorer/)**bold text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhebDJjRKePL"
      },
      "source": [
        "First update the `BUCKET_NAME` parameter below. You can either use an existing bucket or create a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lit30Cktbfvo",
        "outputId": "273ee3ae-cb16-42fd-9d59-898826d2fb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://tuning-demo-erwinh/gemini-tuning\n"
          ]
        }
      ],
      "source": [
        "# Provide a bucket name\n",
        "BUCKET_NAME = \"tuning-demo-erwinh/gemini-tuning\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "print(BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed-G-9cyKmPY"
      },
      "source": [
        "Only run the code below if you want to create a new Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UJ8S9YFA1pZ"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izjwF63tLLEq"
      },
      "source": [
        "Next you will copy the data into your bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjvcxx_sA3xP"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_test.csv .\n",
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_train.csv .\n",
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_validation.csv ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F10LuZeL3kt"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Next you will prepare some test data that you will use to establish a baseline.  This means evaluating your chosen model on a representative sample of your dataset before any fine-tuning. A baseline allows you to quantify the improvements achieved through fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "LkOmXpegA8CW",
        "outputId": "297f4339-83fc-4a4a-9ed0-62a469ac1acd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"5725bae289a1e219009abd92\",\n          \"5726431aec44d21400f3dd13\",\n          \"57269fab5951b619008f7808\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"Teacher\",\n          \"Ctenophora\",\n          \"Normans\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38,\n        \"samples\": [\n          \"On May 21, 2013, NFL owners at their spring meetings in Boston voted and awarded the game to Levi's Stadium. The $1.2 billion stadium opened in 2014. It is the first Super Bowl held in the San Francisco Bay Area since Super Bowl XIX in 1985, and the first in California since Super Bowl XXXVII took place in San Diego in 2003.\",\n          \"The contracted batch of 15 Saturn Vs were enough for lunar landing missions through Apollo 20. NASA publicized a preliminary list of eight more planned landing sites, with plans to increase the mass of the CSM and LM for the last five missions, along with the payload capacity of the Saturn V. These final missions would combine the I and J types in the 1967 list, allowing the CMP to operate a package of lunar orbital sensors and cameras while his companions were on the surface, and allowing them to stay on the Moon for over three days. These missions would also carry the Lunar Roving Vehicle (LRV) increasing the exploration area and allowing televised liftoff of the LM. Also, the Block II spacesuit was revised for the extended missions to allow greater flexibility and visibility for driving the LRV.\",\n          \"In July 1977, General Zia-ul-Haq overthrew Prime Minister Zulfiqar Ali Bhutto's regime in Pakistan. Ali Bhutto, a leftist in democratic competition with Islamists, had announced banning alcohol and nightclubs within six months, shortly before he was overthrown. Zia-ul-Haq was much more committed to Islamism, and \\\"Islamization\\\" or implementation of Islamic law, became a cornerstone of his eleven-year military dictatorship and Islamism became his \\\"official state ideology\\\". Zia ul Haq was an admirer of Mawdudi and Mawdudi's party Jamaat-e-Islami became the \\\"regime's ideological and political arm\\\". In Pakistan this Islamization from above was \\\"probably\\\" more complete \\\"than under any other regime except those in Iran and Sudan,\\\" but Zia-ul-Haq was also criticized by many Islamists for imposing \\\"symbols\\\" rather than substance, and using Islamization to legitimize his means of seizing power. Unlike neighboring Iran, Zia-ul-Haq's policies were intended to \\\"avoid revolutionary excess\\\", and not to strain relations with his American and Persian Gulf state allies. Zia-ul-Haq was killed in 1988 but Islamization remains an important element in Pakistani society.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"How many species of Ctenophores have not been fully described or named?\",\n          \"What was Tymnet\",\n          \"What is Michael Carrick and Alan Shearer's profession?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"possibly another 25\",\n          \"an international data communications network headquartered in San Jose, CA\",\n          \"international footballers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "test_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e2c6c285-cacc-4a88-a773-ac191a7d20e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56de3cd0cffd8e1900b4b6bf</td>\n",
              "      <td>Normans</td>\n",
              "      <td>Normans came into Scotland, building castles a...</td>\n",
              "      <td>What culture's arrival in Scotland is know as ...</td>\n",
              "      <td>Norman</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2c6c285-cacc-4a88-a773-ac191a7d20e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2c6c285-cacc-4a88-a773-ac191a7d20e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2c6c285-cacc-4a88-a773-ac191a7d20e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                         id    title  \\\n",
              "0  56de3cd0cffd8e1900b4b6bf  Normans   \n",
              "\n",
              "                                             context  \\\n",
              "0  Normans came into Scotland, building castles a...   \n",
              "\n",
              "                                            question answers  \n",
              "0  What culture's arrival in Scotland is know as ...  Norman  "
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv('squad_test.csv')\n",
        "test_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLxcVVcMsNO"
      },
      "source": [
        "You will need to do some dataset preperations. We will add a system instruction to the dataset:\n",
        "\n",
        "`SystemInstruct`: System instructions are a set of instructions that the model processes before it processes prompts. We recommend that you use system instructions to tell the model how you want it to behave and respond to prompts.\n",
        "\n",
        "We will also combine the `context` and `question`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "c0pgJycOekZ3"
      },
      "outputs": [],
      "source": [
        "systemInstruct = \"Answer the question based on the context\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_u3VzUMsyqj",
        "outputId": "ff16f581-73de-4595-aeed-6a80b39e8d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the context\n",
            "Context: In July 1973, as part of its outreach programme to young people, the V&A became the first museum in Britain to present a rock concert. The V&A presented a combined concert/lecture by British progressive folk-rock band Gryphon, who explored the lineage of mediaeval music and instrumentation and related how those contributed to contemporary music 500 years later. This innovative approach to bringing young people to museums was a hallmark of the directorship of Roy Strong and was subsequently emulated by some other British museums.\n",
            "Question: Which musical group did the V&A present in July 1973 as part of its youth outreach programme?\n"
          ]
        }
      ],
      "source": [
        "# combine the systeminstruct + context + question into one column.\n",
        "row_dataset = 6\n",
        "\n",
        "test_df[\"input_question\"] = systemInstruct + \"\\n\" + \"Context: \" + test_df[\"context\"] + \"\\n\" + \"Question: \" + test_df[\"question\"]\n",
        "test_question = test_df[\"input_question\"].iloc[row_dataset]\n",
        "print(test_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSxYYqMGWrmj"
      },
      "source": [
        "Next, set the model that you will use. In this example you will use `gemini-1.5-flash-002`. A multimodal model that is designed for high-volume, cost-effective applications, and which delivers speed and efficiency to build fast, lower-cost applications that don't compromise on quality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "t-5X4goiqqBQ"
      },
      "outputs": [],
      "source": [
        "base_model = \"gemini-1.5-flash-002\"\n",
        "generation_model = GenerativeModel(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyscyIenW4WZ"
      },
      "source": [
        "Next lets take a question and get a prediction from Gemini that we can compare to the actual answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "cXencUYc6YAE"
      },
      "outputs": [],
      "source": [
        "def get_predictions(question: str) -> str:\n",
        "  \"\"\"Generates predictions for a given test question.\n",
        "\n",
        "  Args:\n",
        "    test_question: The question to generate predictions for.\n",
        "\n",
        "  Returns:\n",
        "    The generated prediction text.\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = f\"{question}\"\n",
        "\n",
        "  generation_config = GenerationConfig(\n",
        "    temperature=0.1)\n",
        "\n",
        "  response = generation_model.generate_content(\n",
        "      contents=prompt, generation_config=generation_config\n",
        "  ).text\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKa0wLooa3Is",
        "outputId": "ce10cb15-31c9-4fbe-af4d-6c8d65139648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini response: The V&A presented the British progressive folk-rock band Gryphon.\n",
            "\n",
            "Actual answer: Gryphon\n"
          ]
        }
      ],
      "source": [
        "test_answer = test_df[\"answers\"].iloc[row_dataset]\n",
        "\n",
        "response = get_predictions(test_question)\n",
        "\n",
        "print(f\"Gemini response: {response}\")\n",
        "print(f\"Actual answer: {test_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRJTHKrdujw"
      },
      "source": [
        "You can see that both answers are correct, but the response from Gemini is more lengthy. However, answers in the SQuAD dataset are typically concise and clear.\n",
        "\n",
        "Fine-tuning is a great way to control the type of output your use case requires. In this instance, you would want the model to provide short, clear answers.\n",
        "\n",
        "Next, let's check if each dataset has an equal number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCe0CUsi5E-Y",
        "outputId": "8e89fbf4-7483-448e-b50a-4bfd50adeb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of strings in y_pred: 40\n",
            "Number of strings in y_true: 40\n"
          ]
        }
      ],
      "source": [
        "num_strings_pred = np.sum([isinstance(item, str) for item in y_pred])\n",
        "print(f\"Number of strings in y_pred: {num_strings_pred}\")\n",
        "\n",
        "num_strings_true = np.sum([isinstance(item, str) for item in y_true])\n",
        "print(f\"Number of strings in y_true: {num_strings_true}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvi7m8pKE8WB"
      },
      "source": [
        "Next lest establish a baseline using evaluation metrics.\n",
        "\n",
        "Evaluating the performance of a Question Answering (QA) system requires specific metrics. Two commonly used metrics are Exact Match (EM) and F1 score.\n",
        "\n",
        "EM is a strict measure that only considers an answer correct if it perfectly matches the ground truth, even down to the punctuation. It's a binary metric - either 1 for a perfect match or 0 otherwise. This makes it sensitive to minor variations in phrasing.\n",
        "\n",
        "F1 score is more flexible. It considers the overlap between the predicted answer and the true answer in terms of individual words or tokens. It calculates the harmonic mean of precision (proportion of correctly predicted words out of all predicted words) and recall (proportion of correctly predicted words out of all true answer words). This allows for partial credit and is less sensitive to minor wording differences.\n",
        "\n",
        "In practice, EM is useful when exact wording is crucial, while F1 is more suitable when evaluating the overall understanding and semantic accuracy of the QA system. Often, both metrics are used together to provide a comprehensive evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "XcgEpTU55FFc"
      },
      "outputs": [],
      "source": [
        "def calculate_em_and_f1_for_text_arrays(y_true, y_pred, average='weighted'):\n",
        "    \"\"\"\n",
        "    Calculates the Exact Match (EM) and F1 score for arrays of text\n",
        "    using word-level comparisons.\n",
        "\n",
        "    Args:\n",
        "        y_true: An array of ground truth strings.\n",
        "        y_pred: An array of predicted strings.\n",
        "        average: The averaging method to use for F1 score.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the EM score and the F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    em = np.mean([t == p for t, p in zip(y_true, y_pred)])\n",
        "\n",
        "    # Use TF-IDF to convert strings to numerical vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    all_text = np.concatenate((y_true, y_pred))\n",
        "    vectorizer.fit(all_text)\n",
        "    y_true_vec = vectorizer.transform(y_true)\n",
        "    y_pred_vec = vectorizer.transform(y_pred)\n",
        "\n",
        "    # Calculate F1 score based on common words (non-zero elements)\n",
        "    y_true_class = (y_true_vec > 0).toarray().astype(int)\n",
        "    y_pred_class = (y_pred_vec > 0).toarray().astype(int)\n",
        "\n",
        "    f1 = f1_score(y_true_class, y_pred_class, average=average)\n",
        "\n",
        "    return em, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhDTq9p_GSBP",
        "outputId": "4dde775e-2466-4ef7-e380-d23abddc6690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM score: 0.0\n",
            "F1 score: 0.030862136294937427\n"
          ]
        }
      ],
      "source": [
        "em, f1 = calculate_em_and_f1_for_text_arrays(y_pred, y_true)\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22DfexbNfUHm"
      },
      "source": [
        "### Prepare the data for fine-tuning\n",
        "\n",
        "To optimize the tuning process for a foundation model, ensure your dataset includes examples that reflect the desired task. Structure your training data in a text-to-text format, where each record in the dataset pairs an input text (or prompt) with its corresponding expected output. This supervised tuning approach uses the dataset to effectively teach the model the specific behavior or task you need it to perform, by providing numerous illustrative examples.\n",
        "\n",
        "The size of your dataset will vary depending on the complexity of the task, but as a general rule, the more examples you include, the better the model's performance.\n",
        "\n",
        "Dataset Format\n",
        "Your training data should be structured in a JSONL file and stored at a Google Cloud Storage (GCS) URI.  Each line in the JSONL file must adhere to the following schema:\n",
        "\n",
        "A `contents` array containing objects that define:\n",
        "- A `role` (\"user\" for user input or \"model\" for model output)\n",
        "- `parts` containing the input data.\n",
        "\n",
        "```\n",
        "{\n",
        "   \"contents\":[\n",
        "      {\n",
        "         \"role\":\"user\",  # This indicate input content\n",
        "         \"parts\":[\n",
        "            {\n",
        "               \"text\":\"How are you?\"\n",
        "            }\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"role\":\"model\", # This indicate target content\n",
        "         \"parts\":[ # text only\n",
        "            {\n",
        "               \"text\":\"I am good, thank you!\"\n",
        "            }\n",
        "         ]\n",
        "      }\n",
        "      #  ... repeat \"user\", \"model\" for multi turns.\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "Refer to the public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare#about-datasets) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "4DqrQp4cLqRy"
      },
      "outputs": [],
      "source": [
        "# combine the systeminstruct + context + question into one column.\n",
        "train_df = pd.read_csv('squad_train.csv')\n",
        "validation_df = pd.read_csv('squad_validation.csv')\n",
        "\n",
        "train_df[\"input_question\"] = systemInstruct + \"Context: \" + train_df[\"context\"] + \"Question: \" + train_df[\"question\"]\n",
        "validation_df[\"input_question\"] = systemInstruct + \"Context: \" + validation_df[\"context\"] + \"Question: \" + validation_df[\"question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmzyz1migvHN",
        "outputId": "38b0b753-b526-41a8-d124-d73baa2152bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONL data written to squad_train.jsonl\n",
            "JSONL data written to squad_validation.jsonl\n"
          ]
        }
      ],
      "source": [
        "def df_to_jsonl(df, output_file):\n",
        "  \"\"\"Converts a Pandas DataFrame to JSONL format and saves it to a file.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame to convert.\n",
        "    output_file: The name of the output file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(output_file, 'w') as f:\n",
        "    for row in df.itertuples(index=False):\n",
        "      jsonl_obj = {\n",
        "          \"systemInstruction\": {\"parts\": [{\"text\": \"Answer the question based on the provided context.\"}]},\n",
        "          \"contents\": [\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"parts\": [{\"text\": f\"Context: {row.context}\\n\\nQuestion: {row.question}\"}]\n",
        "              },\n",
        "              {\"role\": \"model\", \"parts\": [{\"text\": row.answers}]},\n",
        "          ]\n",
        "      }\n",
        "      f.write(json.dumps(jsonl_obj) + '\\n')\n",
        "\n",
        "# Process the DataFrames\n",
        "df_to_jsonl(train_df, 'squad_train.jsonl')\n",
        "df_to_jsonl(validation_df, 'squad_validation.jsonl')\n",
        "\n",
        "print(f\"JSONL data written to squad_train.jsonl\")\n",
        "print(f\"JSONL data written to squad_validation.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQv-ZMpJDhi"
      },
      "source": [
        "Next you will copy the files into your Google Cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5k1jYJ10IeW",
        "outputId": "f6af525a-0c69-414b-a9f7-7340879f4868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://./squad_train.jsonl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][527.0 KiB/527.0 KiB]                                                \n",
            "Operation completed over 1 objects/527.0 KiB.                                    \n",
            "Copying file://./squad_validation.jsonl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][110.9 KiB/110.9 KiB]                                                \n",
            "Operation completed over 1 objects/110.9 KiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp ./squad_train.jsonl {BUCKET_URI}\n",
        "!gsutil cp ./squad_validation.jsonl {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAHMYgFJJHjm"
      },
      "source": [
        "### Start fine-tuning job\n",
        "Next you can start the fine-tuning job.\n",
        "\n",
        "- `source_model`: Specifies the base Gemini model version you want to fine-tune.\n",
        " - `train_dataset`: Path to your training data in JSONL format.\n",
        "\n",
        "  *Optional parameters*\n",
        " - `validation_dataset`: If provided, this data is used to evaluate the model during tuning.\n",
        " - `tuned_model_display_name`: Display name for the tuned model.\n",
        " - `epochs`: The number of training epochs to run.\n",
        " - `learning_rate_multiplier`: A value to scale the learning rate during training.\n",
        " - `adapter_size` : Gemini 1.5 Flash supports Adapter length [1, 4], default value is 4.\n",
        "\n",
        " **Important**: The default hyperparameter settings are optimized for optimal performance based on rigorous testing and are recommended for initial use. Users may customize these parameters to address specific performance requirements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "qj-LjQ5Vbf1E",
        "outputId": "5af1f956-d5e3-4111-c100-85e60cc90890"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:Creating SupervisedTuningJob\n",
            "INFO:vertexai.tuning._tuning:SupervisedTuningJob created. Resource name: projects/713601331534/locations/us-central1/tuningJobs/8356726629560483840\n",
            "INFO:vertexai.tuning._tuning:To use this SupervisedTuningJob in another session:\n",
            "INFO:vertexai.tuning._tuning:tuning_job = sft.SupervisedTuningJob('projects/713601331534/locations/us-central1/tuningJobs/8356726629560483840')\n",
            "INFO:vertexai.tuning._tuning:View Tuning Job:\n",
            "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/8356726629560483840?project=713601331534\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-e4cd5e19-6c60-439c-8aff-5f8b0dfeb3c0\" href=\"#view-view-vertex-resource-e4cd5e19-6c60-439c-8aff-5f8b0dfeb3c0\">\n",
              "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
              "          <span>View Tuning Job</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-e4cd5e19-6c60-439c-8aff-5f8b0dfeb3c0');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/8356726629560483840?project=713601331534');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/8356726629560483840?project=713601331534', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tuned_model_display_name = \"erwinh-fine-tuning-flash\"  # @param {type:\"string\"}\n",
        "\n",
        "sft_tuning_job = sft.train(\n",
        "    source_model=base_model,\n",
        "    train_dataset=f\"\"\"{BUCKET_URI}/squad_train.jsonl\"\"\",\n",
        "    # # Optional:\n",
        "    validation_dataset=f\"\"\"{BUCKET_URI}/squad_validation.jsonl\"\"\",\n",
        "    tuned_model_display_name=tuned_model_display_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tXawW1p8E5-",
        "outputId": "83b11366-6ff2-4900-d6af-3a0f7a9aca19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'projects/713601331534/locations/us-central1/tuningJobs/8356726629560483840',\n",
              " 'tunedModelDisplayName': 'erwinh-fine-tuning-flash',\n",
              " 'baseModel': 'gemini-1.5-flash-002',\n",
              " 'supervisedTuningSpec': {'trainingDatasetUri': 'gs://tuning-demo-erwinh/gemini-tuning/squad_train.jsonl',\n",
              "  'validationDatasetUri': 'gs://tuning-demo-erwinh/gemini-tuning/squad_validation.jsonl',\n",
              "  'hyperParameters': {'epochCount': '10',\n",
              "   'learningRateMultiplier': 1.0,\n",
              "   'adapterSize': 'ADAPTER_SIZE_EIGHT'}},\n",
              " 'state': 'JOB_STATE_SUCCEEDED',\n",
              " 'createTime': '2024-10-09T06:20:11.698883Z',\n",
              " 'startTime': '2024-10-09T06:20:11.740358Z',\n",
              " 'endTime': '2024-10-09T06:32:38.867719Z',\n",
              " 'updateTime': '2024-10-09T06:32:38.867719Z',\n",
              " 'experiment': 'projects/713601331534/locations/us-central1/metadataStores/default/contexts/tuning-experiment-20241008232013040864',\n",
              " 'tunedModel': {'model': 'projects/713601331534/locations/us-central1/models/1582035604160380928@1',\n",
              "  'endpoint': 'projects/713601331534/locations/us-central1/endpoints/5693131570647400448'},\n",
              " 'tuningDataStats': {'supervisedTuningDataStats': {'tuningDatasetExampleCount': '500',\n",
              "   'userInputTokenDistribution': {'sum': '94474',\n",
              "    'min': 54.0,\n",
              "    'max': 602.0,\n",
              "    'mean': 188.948,\n",
              "    'median': 172.0,\n",
              "    'p5': 107.0,\n",
              "    'p95': 327.0,\n",
              "    'buckets': [{'count': 126.0, 'left': 54.0, 'right': 145.0},\n",
              "     {'count': 273.0, 'left': 146.0, 'right': 236.0},\n",
              "     {'count': 77.0, 'left': 237.0, 'right': 328.0},\n",
              "     {'count': 21.0, 'left': 329.0, 'right': 419.0},\n",
              "     {'count': 2.0, 'left': 420.0, 'right': 511.0},\n",
              "     {'count': 1.0, 'left': 512.0, 'right': 602.0}]},\n",
              "   'userOutputTokenDistribution': {'sum': '2420',\n",
              "    'min': 1.0,\n",
              "    'max': 43.0,\n",
              "    'mean': 4.84,\n",
              "    'median': 4.0,\n",
              "    'p5': 1.0,\n",
              "    'p95': 15.0,\n",
              "    'buckets': [{'count': 440.0, 'left': 1.0, 'right': 8.0},\n",
              "     {'count': 38.0, 'left': 9.0, 'right': 15.0},\n",
              "     {'count': 16.0, 'left': 16.0, 'right': 22.0},\n",
              "     {'count': 4.0, 'left': 23.0, 'right': 29.0},\n",
              "     {'count': 1.0, 'left': 30.0, 'right': 36.0},\n",
              "     {'count': 1.0, 'left': 37.0, 'right': 43.0}]},\n",
              "   'userMessagePerExampleDistribution': {'sum': '1000',\n",
              "    'min': 2.0,\n",
              "    'max': 2.0,\n",
              "    'mean': 2.0,\n",
              "    'median': 2.0,\n",
              "    'p5': 2.0,\n",
              "    'p95': 2.0,\n",
              "    'buckets': [{'count': 500.0, 'left': 2.0, 'right': 2.0}]},\n",
              "   'userDatasetExamples': [{'role': 'user',\n",
              "     'parts': [{'text': 'Context: On the next day, December 18, protests turned into civil unrest as clashes between troops, volunteers, militia units, and Kazakh students turned into a wide-scale confrontation. The clashes could only be controlled on the third day. The Almaty events were followed by smaller protests and demonstrations in Shymkent, Pavlodar, Karaganda, and Taldykorgan. Reports from Kazakh SSR authoriti...'}]},\n",
              "    {'role': 'model', 'parts': [{'text': '3,000'}]},\n",
              "    {'role': 'user',\n",
              "     'parts': [{'text': \"Context: Roman Catholicism was the sole established religion in the Holy Roman Empire until the Reformation changed this drastically. In 1517, Martin Luther challenged the Catholic Church as he saw it as a corruption of Christian faith. Through this, he altered the course of European and world history and established Protestantism. The Thirty Years' War (1618–1648) was one of the most destructi...\"}]},\n",
              "    {'role': 'model', 'parts': [{'text': 'Roman Catholicism'}]},\n",
              "    {'role': 'user',\n",
              "     'parts': [{'text': \"Context: Israel retaliated against Egyptian shelling with commando raids, artillery shelling and air strikes. This resulted in an exodus of civilians from Egyptian cities along the Suez Canal's western bank. Nasser ceased all military activities and began a program to build a network of internal defenses, while receiving the financial backing of various Arab states. The war resumed in March 196...\"}]},\n",
              "    {'role': 'model', 'parts': [{'text': 'March 1969'}]}],\n",
              "   'totalBillableTokenCount': '96894'}}}"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the tuning job info.\n",
        "sft_tuning_job.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "19aQnN-k84d9",
        "outputId": "5c7f8c4f-566f-4c12-ffc7-60b4340d03d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'projects/713601331534/locations/us-central1/tuningJobs/8356726629560483840'"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the resource name of the tuning job\n",
        "sft_tuning_job_name = sft_tuning_job.resource_name\n",
        "sft_tuning_job_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKo8cwF2KVM5"
      },
      "source": [
        "**Important:** Tuning time depends on several factors, such as training data size, number of epochs, learning rate multiplier, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NiZnPkIKcwm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~30 mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njag_3cB86rH",
        "outputId": "b1408519-3735-4aca-86aa-89b0da5699b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 67.5 ms, sys: 4.98 ms, total: 72.5 ms\n",
            "Wall time: 1.28 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Wait for job completion\n",
        "while not sft_tuning_job.refresh().has_ended:\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dkx92RBdbf27",
        "outputId": "0641746b-de08-4f9b-80cc-5afef558e7ea"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'projects/713601331534/locations/us-central1/models/1582035604160380928@1'"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tuned model name\n",
        "tuned_model_name = sft_tuning_job.tuned_model_name\n",
        "tuned_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e09aB_9Ebf5c",
        "outputId": "c8a028c1-da5f-4e7d-d9db-ecde9c5293e1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'projects/713601331534/locations/us-central1/endpoints/5693131570647400448'"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tuned model endpoint name\n",
        "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
        "tuned_model_endpoint_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1ukBznKmlN"
      },
      "source": [
        "#### Model tuning metrics\n",
        "\n",
        "- `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
        "- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
        "- `/train_num_predictions`: Number of predicted tokens at a training step\n",
        "\n",
        "#### Model evaluation metrics:\n",
        "\n",
        "- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n",
        "- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n",
        "- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\n",
        "\n",
        "The metrics visualizations are available after the model tuning job completes. If you don't specify a validation dataset when you create the tuning job, only the visualizations for the tuning metrics are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "DH0guHM---Jo"
      },
      "outputs": [],
      "source": [
        "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
        "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
        "filter_str = metadata_utils._make_filter_string(\n",
        "    schema_title=\"system.ExperimentRun\",\n",
        "    parent_contexts=[experiment.resource_name],\n",
        ")\n",
        "experiment_run = context.Context.list(filter_str)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "hggHQFIl_FXC"
      },
      "outputs": [],
      "source": [
        "# Read data from Tensorboard\n",
        "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
        "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
        "metrics = tensorboard_run.read_time_series_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "BdHKZdqG_bHf"
      },
      "outputs": [],
      "source": [
        "def get_metrics(metric: str = \"/train_total_loss\"):\n",
        "    \"\"\"\n",
        "    Get metrics from Tensorboard.\n",
        "\n",
        "    Args:\n",
        "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
        "    Returns:\n",
        "      steps: list of steps.\n",
        "      steps_loss: list of loss values.\n",
        "    \"\"\"\n",
        "    loss_values = metrics[metric].values\n",
        "    steps_loss = []\n",
        "    steps = []\n",
        "    for loss in loss_values:\n",
        "        steps_loss.append(loss.scalar.value)\n",
        "        steps.append(loss.step)\n",
        "    return steps, steps_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "_pDrlpA7_e9o"
      },
      "outputs": [],
      "source": [
        "# Get Train and Eval Loss\n",
        "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
        "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "DL07j7u__iZx",
        "outputId": "3cff463b-59f7-4db3-a884-e3a099583ab5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d44fcf4e-7d0a-4fa0-b15d-30d3662c751c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d44fcf4e-7d0a-4fa0-b15d-30d3662c751c\")) {                    Plotly.newPlot(                        \"d44fcf4e-7d0a-4fa0-b15d-30d3662c751c\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67],\"y\":[7.4884819984436035,7.528019905090332,6.837996482849121,6.68577241897583,7.8964409828186035,7.400771141052246,8.076016426086426,6.312904357910156,6.624507427215576,5.649761199951172,5.365241050720215,3.3854150772094727,2.6788747310638428,2.0648250579833984,1.0480821132659912,0.6648333072662354,0.47732076048851013,0.4093046188354492,0.26741915941238403,0.4671071469783783,0.4113951623439789,0.36876198649406433,0.376609206199646,0.25337275862693787,0.2808288633823395,0.3060764968395233,0.23812375962734222,0.24310339987277985,0.2612179219722748,0.23198972642421722,0.30561214685440063,0.24862012267112732,0.21123315393924713,0.178488627076149,0.22334438562393188,0.18150323629379272,0.21154151856899261,0.16323335468769073,0.1777341067790985,0.17672231793403625,0.23767749965190887,0.14071762561798096,0.1825181096792221,0.20686523616313934,0.1275780349969864,0.11179222166538239,0.20213821530342102,0.1731559932231903,0.18120594322681427,0.11931592971086502,0.10117621719837189,0.1511683166027069,0.12464357167482376,0.14280998706817627,0.14177025854587555,0.1147642657160759,0.10881330817937851,0.1299658566713333,0.13697023689746857,0.10764651745557785,0.11950445175170898,0.10593226552009583,0.11007212847471237,0.0670647844672203,0.12377403676509857,0.1485866755247116,0.0944487676024437],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Eval Loss\",\"x\":[1,67],\"y\":[8.69354248046875,0.22064246237277985],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Steps\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Steps\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Train Loss\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Eval Loss\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Train and Eval Loss\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d44fcf4e-7d0a-4fa0-b15d-30d3662c751c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the train and eval loss metrics using Plotly python library\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
        ")\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=1,\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=2,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Steps\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYygz5ph_icf",
        "outputId": "f15d6aa8-bbf3-46ca-e919-e91760c9ba90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***Testing***\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"European Council\\n\\n\"\n",
            "    }\n",
            "  }\n",
            "  finish_reason: STOP\n",
            "  avg_logprobs: -0.11596920092900594\n",
            "}\n",
            "usage_metadata {\n",
            "  prompt_token_count: 290\n",
            "  candidates_token_count: 3\n",
            "  total_token_count: 293\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\n",
        "    # Test with the loaded model.\n",
        "    print(\"***Testing***\")\n",
        "    print(\n",
        "        tuned_genai_model.generate_content(\n",
        "            contents=prompt, generation_config=generation_config\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    print(\"State:\", sft_tuning_job.state)\n",
        "    print(\"Error:\", sft_tuning_job.error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbTaqCMxNf18"
      },
      "source": [
        "### Model usage and evaluation.\n",
        "\n",
        "Next you will evaluate the model to see how well it performs. You can also compare it to the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "W4YMNGuoDajB"
      },
      "outputs": [],
      "source": [
        "y_true = test_df[\"answers\"].values\n",
        "\n",
        "def get_predictions(test_question):\n",
        "\n",
        "  prompt = f\"\"\"{test_question}\"\"\"\n",
        "\n",
        "  generation_config = GenerationConfig(\n",
        "      temperature=0.1,\n",
        "  )\n",
        "\n",
        "  response = tuned_genai_model.generate_content(contents=prompt, generation_config=generation_config).text\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "69FMuAeoDrm5"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "y_pred_question = test_df[\"question\"].values\n",
        "\n",
        "for i in y_pred_question:\n",
        "\n",
        "  prediction = get_predictions(i)\n",
        "  y_pred.append(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj76Tu6ODalZ",
        "outputId": "f5d76e1d-1fbf-4251-9c19-4f430a97ad0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM score: 0.0\n",
            "F1 score: 0.2399679487179487\n"
          ]
        }
      ],
      "source": [
        "em, f1 = calculate_em_and_f1_for_text_arrays(y_pred, y_true)\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkb2qXljFYqX"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
