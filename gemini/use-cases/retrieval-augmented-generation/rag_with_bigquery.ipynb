{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Run RAG Pipelines in BigQuery with BQML and Vector Search\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/rag_with_bigquery.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Frag_with_bigquery.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/rag_with_bigquery.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/rag_with_bigquery.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Jeff Nelson](https://github.com/jeffonelson/), Eric Hao |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates a basic end-to-end retrieval-augmented generation (RAG) pipeline using BigQuery and BigQuery ML functions. To do so, we:\n",
        "\n",
        "* Complete setup steps to download sample data and access Vertex AI from BigQuery\n",
        "* Generate object table to access unstructured PDFs in Cloud Storage\n",
        "* Create a remote model, so BigQuery can Document AI and parse the input documents\n",
        "* Parse the data from Document AI into chunks and metadata, then generate vector embeddings for them\n",
        "* Run a vector search against embeddings in BigQuery, return relevant chunks, and summarize them with Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to open this notebook in BigQuery Studio\n",
        "\n",
        "This notebook was written to be compatible for use within BigQuery Studio. To open this notebook in BigQuery, click to [Run in Colab Enterprise](https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Frag_with_bigquery.ipynb). This will open a new window in the Cloud Console and prompt you to confirm import. Then, navigate to BigQuery, where you will find the notebook available in the Explorer pane under Notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About the dataset\n",
        "\n",
        "This example uses [Alphabet's 2023 10-K](https://abc.xyz/assets/43/44/675b83d7455885c4615d848d52a4/goog-10-k-2023.pdf) form. This is a detailed overview of the company's financial information and includes text, tables, and diagrams spanning nearly 100 pages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Services and Costs\n",
        "\n",
        "This tutorial uses the following Google Cloud data analytics and ML services, they are billable components of Google Cloud:\n",
        "\n",
        "* BigQuery & BigQuery ML [(pricing)](https://cloud.google.com/bigquery/pricing)\n",
        "* Vertex AI Generative AI models [(pricing)](https://cloud.google.com/vertex-ai/generative-ai/pricing)\n",
        "\n",
        "Use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "# Setup Steps to access Vertex AI models from BigQuery and enable APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Document AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --user --quiet google-cloud-documentai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Define your Google Cloud project (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = 'your-project-id' # @param {type: \"string\"}\n",
        "PROJECT_NUMBER = 'your-project-number' # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enable Data Table Display\n",
        "\n",
        "This makes it easier to visualize tabluar data within a Notebook environment later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a new dataset in BigQuery\n",
        "\n",
        "This will house any tables created throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bq mk --location=us --dataset --project_id={PROJECT_ID} docai_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Cloud resource connection\n",
        "\n",
        "[Cloud resource connections](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection) enable BigQuery to access other Cloud services, like Cloud Storage and Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bq mk --connection --connection_type=CLOUD_RESOURCE --location=us --project_id={PROJECT_ID} \"demo_conn\"\n",
        "!bq show --location=us --connection --project_id={PROJECT_ID} \"demo_conn\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add permissions to Cloud resource connection service account\n",
        "\n",
        "The Cloud resource connection is assocaited with a service account. The following cell enables the service account to access services like Document AI, Cloud Storage, and Vertex AI.\n",
        "\n",
        "**Note:** copy the service account ID from the prior cell and input it below. It will look like `your-copied-service-account@gcp-sa-bigquery-condel.iam.gserviceaccount.com`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_service_account = 'your-copied-service-account@gcp-sa-bigquery-condel.iam.gserviceaccount.com' # @param {type: \"string\"}\n",
        "connection_member = f'serviceAccount:{connection_service_account}'\n",
        "\n",
        "\n",
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} --member={connection_member} --role='roles/documentai.viewer' --condition=None --quiet\n",
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} --member={connection_member} --role='roles/storage.objectViewer' --condition=None --quiet\n",
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} --member={connection_member} --role='roles/aiplatform.user' --condition=None --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an object table\n",
        "\n",
        "An object table allows BigQuery to read unstructured data in Google Cloud Storage.\n",
        "\n",
        "Input your Google Cloud Storage bucket in the `uris` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID --location us\n",
        "\n",
        "CREATE OR REPLACE EXTERNAL TABLE docai_demo.object_table\n",
        "WITH CONNECTION `us.demo_conn`\n",
        "OPTIONS(\n",
        "  object_metadata=\"DIRECTORY\",\n",
        "  uris=[\"gs://YOUR-GCS-BUCKET/goog-10-k-2023.pdf\"]\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show the object table\n",
        "\n",
        "Confirm that the results display the PDF document in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "SELECT * \n",
        "FROM `docai_demo.object_table`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use BQML and Document AI to parse documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Layour Parser Processor in Document AI\n",
        "\n",
        "[Create a new processor](https://cloud.google.com/document-ai/docs/create-processor#documentai_fetch_processor_types-python) in Document AI with the type `LAYOUT_PARSER_PROCESSOR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import documentai\n",
        "\n",
        "location = 'us'\n",
        "processor_display_name = 'docai_layout_parser_processor'\n",
        "processor_type = 'LAYOUT_PARSER_PROCESSOR'\n",
        "\n",
        "\n",
        "def create_processor_sample(\n",
        "    PROJECT_ID: str, location: str, processor_display_name: str, processor_type: str\n",
        ") -> None:\n",
        "    \n",
        "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
        "\n",
        "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
        "\n",
        "    # The full resource name of the location\n",
        "    parent = client.common_location_path(PROJECT_ID, location)\n",
        "\n",
        "    # Create a processor\n",
        "    processor = client.create_processor(\n",
        "        parent=parent,\n",
        "        processor=documentai.Processor(\n",
        "            display_name=processor_display_name, type_=processor_type\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Return the processor ID needed for creating a BigQuery connection\n",
        "    return processor.name.split('/')[-1]\n",
        "\n",
        "# Call this function to create the processor and return its ID\n",
        "create_processor_sample(PROJECT_ID, location, processor_display_name, processor_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a remote model in BigQuery that references your Document AI Layout Parser Processor\n",
        "\n",
        "This one-time setup step allows BigQuery to reference the Document AI Processor you just set up.\n",
        "\n",
        "**Note:** please copy the processor ID from the prior step and add it below in the `document_processor` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID --location us\n",
        "\n",
        "CREATE OR REPLACE MODEL\n",
        "  `docai_demo.layout_parser` REMOTE WITH CONNECTION `us.demo_conn`\n",
        "  OPTIONS(remote_service_type=\"CLOUD_AI_DOCUMENT_V1\", document_processor=\"YOUR_PROCESSOR_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the document using BigQuery ML\n",
        "\n",
        "Use the `ML.PROCESS_DOCUMENT` function to call your Processor in Document AI and pass through the PDF. This uses the Layout Parser configuration and chunks your document.\n",
        "\n",
        "**Note:** this may take a minute or so to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID --location us\n",
        "\n",
        "CREATE or REPLACE TABLE docai_demo.demo_result AS (\n",
        "  SELECT * FROM ML.PROCESS_DOCUMENT(\n",
        "  MODEL docai_demo.layout_parser,\n",
        "  TABLE docai_demo.object_table,\n",
        "  PROCESS_OPTIONS => (JSON '{\"layout_config\": {\"chunking_config\": {\"chunk_size\": 250}}}')\n",
        "  )\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse the JSON results returned to BigQuery\n",
        "\n",
        "The `ML.PROCESS_DOCUMENT` function parses the PDF from Cloud Storage and returns a JSON blob to BigQuery. In this step, we'll parse the JSON, extract document chunks and metadata, and return it to a new BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID --location us\n",
        "\n",
        "CREATE OR REPLACE TABLE docai_demo.demo_result_parsed AS (\n",
        "SELECT\n",
        "  uri,\n",
        "  JSON_EXTRACT_SCALAR(json , '$.chunkId') AS id,\n",
        "  JSON_EXTRACT_SCALAR(json , '$.content') AS content,\n",
        "  JSON_EXTRACT_SCALAR(json , '$.pageFooters[0].text') AS page_footers_text,\n",
        "  JSON_EXTRACT_SCALAR(json , '$.pageSpan.pageStart') AS page_span_start,\n",
        "  JSON_EXTRACT_SCALAR(json , '$.pageSpan.pageEnd') AS page_span_end\n",
        "FROM docai_demo.demo_result, UNNEST(JSON_EXTRACT_ARRAY(ml_process_document_result.chunkedDocument.chunks, '$')) json\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display the parsed document chunks\n",
        "\n",
        "Show a preview of the document chunks and metadata returned from Doc AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2523377552.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    SELECT *\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "%%bigquery --project $PROJECT_ID --location us\n",
        "\n",
        "SELECT *\n",
        "FROM docai_demo.demo_results_parsed\n",
        "ORDER BY id;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to Vertex AI embedding generation and Gemini access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to a text embedding model\n",
        "\n",
        "Create a remote model allowing BigQuery access to a text embedding model hosted in Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "CREATE OR REPLACE MODEL `docai_demo.embedding_model` \n",
        "REMOTE WITH CONNECTION `us.demo_conn` OPTIONS(endpoint=\"text-embedding-004\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate embeddings\n",
        "\n",
        "Use the `ML.GENERATE_EMBEDDING` function in BigQuery to generate embeddings for all text chunks in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "CREATE OR REPLACE TABLE `docai_demo.embeddings` AS\n",
        "SELECT * FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `docai_demo.embedding_model`,\n",
        "  TABLE `docai_demo.demo_result_parsed`\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to a Gemini LLM endpoint\n",
        "\n",
        "Create a remote model allowing BigQuery access to a Gemini foundation model hosted in Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "CREATE OR REPLACE MODEL `docai_demo.gemini_1.5_flash` REMOTE\n",
        "WITH CONNECTION `us.demo_conn` OPTIONS(endpoint=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run vector search and augment Gemini with text for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample BigQuery vector search\n",
        "\n",
        "Run a sample BigQuery vector search against your chunks. It takes your text input, creates an embedding using the `ML.GENERATE_EMBEDDING` function, and then passes the result through to the `VECTOR_SEARCH` function. The results are the top ten chunks that closest matched your input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "SELECT query.query, base.uri, base.id, base.content, distance\n",
        "    FROM\n",
        "      VECTOR_SEARCH( TABLE `docai_demo.embeddings`,\n",
        "        'ml_generate_embedding_result',\n",
        "        (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result,\n",
        "          content AS query\n",
        "        FROM\n",
        "          ML.GENERATE_EMBEDDING( MODEL `docai_demo.embedding_model`,\n",
        "            ( SELECT 'What was Alphabets revenue in 2023?' AS content)\n",
        "          ) \n",
        "        ),\n",
        "        top_k => 10,\n",
        "        OPTIONS => '{\"fraction_lists_to_search\": 0.01}') \n",
        "ORDER BY distance DESC;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate text augmented by vector search results\n",
        "\n",
        "This step builds upon the prior one - but instead of simply returning the top text chunks, it calls the `ML.GENERATE_TEXT` function to summarize them alongside the question we input.\n",
        "\n",
        "In this step you:\n",
        "* **Retrieve** the closest chunks semantically using the `VECTOR_SEARCH` function\n",
        "* **Augment** the Gemini LLM with this knowledge\n",
        "* **Generate** a succinct answer using the `ML.GENERATE_TEXT` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SELECT\n",
        "  ml_generate_text_llm_result AS generated,\n",
        "  prompt\n",
        "FROM\n",
        "  ML.GENERATE_TEXT( MODEL `docai_demo.gemini`,\n",
        "    (\n",
        "    SELECT\n",
        "    CONCAT( 'What is yearly revuene for google in the last three years? Use the context and mention the reference file used in the answer: ',\n",
        "    STRING_AGG(FORMAT(\"context: %s and reference: %s\", base.content, base.uri), ',\\n')) AS prompt,\n",
        "    FROM\n",
        "      VECTOR_SEARCH( TABLE \n",
        "        `docai_demo.embeddings`,\n",
        "        'ml_generate_embedding_result',\n",
        "        (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result,\n",
        "          content AS query\n",
        "        FROM\n",
        "          ML.GENERATE_EMBEDDING( MODEL `docai_demo.embedding_model`,\n",
        "            (\n",
        "            SELECT\n",
        "              'revenue' AS content\n",
        "            )\n",
        "          ) \n",
        "        ),\n",
        "        top_k => 10,\n",
        "        OPTIONS => '{\"fraction_lists_to_search\": 0.01}') \n",
        "      ),\n",
        "      STRUCT(512 AS max_output_tokens, TRUE AS flatten_json_output)\n",
        "  );\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial by uncommenting the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# !bq rm -r -f $PROJECT_ID:docai_demo\n",
        "# !bq rm --connection --project_id=$PROJECT_ID --location=us demo_conn\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wrap up\n",
        "\n",
        "In this you have seen an example of how to integrate BQML with Vertex AI LLMs, and given examples of how the `ML.GENERATE_TEXT` function can be applied directly to multimodal data stored in BigQuery, as well as how to generate embeddings with `ML.GENERATE_EMBEDDING`.\n",
        "\n",
        "Check out our BigQuery ML documentation on [generating text](https://cloud.google.com/bigquery/docs/generate-text) and [generating embeddings](https://cloud.google.com/bigquery/docs/generate-text-embedding) to learn more about generative AI in BigQuery."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_template.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
